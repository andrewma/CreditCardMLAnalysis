---
title: "Hyperparameter tuning"
output: html_document
---

```{r}
rm(list=ls())
library(MASS)
library(dplyr)
library(xgboost)
```

```{r}
set.seed(1)
boston_train = Boston %>%
  sample_frac(.5)

boston_test = Boston %>%
  setdiff(boston_train)
```

```{r}
Y_train <- as.matrix(boston_train[,"medv"])
X_train <- as.matrix(boston_train[!names(boston_train) %in% c("medv")])
dtrain <- xgb.DMatrix(data = X_train, label = Y_train)

X_test <- as.matrix(boston_test[!names(boston_train) %in% c("medv")])
Y_test <- as.matrix(boston_test[,"medv"])
dtest <- xgb.DMatrix(data = X_test, label = Y_test)
```

```{r}
set.seed(1)
params <- list(max_depth = 5,
               eta = 0.1,
               lambda = 0, # coef of the L2 regularization
               objective = "reg:linear")

boston.xgb = xgboost(param = params, data=dtrain,
                     nrounds = 91,
                     print_every_n = 10)
```

```{r}
yhat.xgb <- predict(boston.xgb,X_test)
round(mean((yhat.xgb - boston_test$medv)^2),2)
```

```{r}
# create hyperparameter grid
nrounds = 60
hyper_grid <- expand.grid(
    eta = seq(2,30,5)/nrounds,
    #subsample = 1.0,
    #colsample_bytree = 1.0,
    max_depth = c(1,2,3,4,5,6),
    #gamma = 1, #Minimum loss reduction required to make a further partition on a leaf node of the tree
    #min_child_weight = 1,
    lambda = 0, # coef of the L2 regularization
    optimal_trees = 0,  # a place to dump results
    min_RMSE = 0        # a place to dump results         
)

nrow(hyper_grid)
```

```{r}
for(i in 1:nrow(hyper_grid)) {
  
  set.seed(1)
  cv.nround = 200
  cv.nfold = 3  
params = list(
    eta = hyper_grid$eta[i], 
    #subsample = 1.0,
    #colsample_bytree = 1.0,
    max_depth = hyper_grid$max_depth[i],
    lambda = 0 
    #gamma = 1,
    #min_child_weight = 1
)
    
  # train model
  boston.xgb.cv <- xgb.cv(param=params, data = dtrain,
                        nfold = cv.nfold,
                        nrounds=cv.nround,
                        early_stopping_rounds = 20, # training will stop if performance doesn't improve for 20 rounds from the last best iteration
                        verbos = 0 #do not show the iterations' results
                        )  
    
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- boston.xgb.cv$best_iteration
  hyper_grid$min_RMSE[i] <- boston.xgb.cv$evaluation_log$test_rmse_mean[boston.xgb.cv$best_iteration]
}
```

```{r}
hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r}
params = list(
  eta = 0.2, #eta = c(0.03 0.11 0.2 0.28 0.36 0.45),
  #subsample = 1.0,
  #colsample_bytree = 1.0,
  max_depth = 2
  #gamma = 1,
  #min_child_weight = 1
)

watchlist <- list(train = dtrain, test = dtest)
boston.xgb = xgb.train(param = params, data=dtrain,
                       nrounds = 85,
                       print_every_n = 10,
                       watchlist = watchlist)

yhat.xgb <- predict(boston.xgb,X_test)
round(mean((yhat.xgb - boston_test$medv)^2),2)
```

```{r}
library(gbm)
```

```{r}
set.seed(1)

yhat.boost <- gbm(medv~., data = boston_train,
                    distribution = "gaussian",
                    n.trees= 100,
                    interaction.depth=4,
                    shrinkage = 0.01, #learning rate
                    verbose=F)

yhat.boost <- predict(yhat.boost, boston_test, n.trees = 100)
mean((yhat.boost-boston_test$medv)^2)
```

```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(2, 4, 6),
  n.minobsinnode = c(5, 10, 15), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid)
```

```{r}
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(1)
  
  # train model
  gbm.tune <- gbm(
    formula = medv ~ .,
    distribution = "gaussian",
    data = boston_train,
    n.trees = 2000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    train.fraction = .75, # train only on 0.75% of train subset and evaluate on remaining 0.25%
    n.cores = NULL # will use all cores by default
    #verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error)) 
}
```

```{r}
hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r}
# modify hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.05, .1, .2),
  interaction.depth = c(4, 6, 8),
  n.minobsinnode = c(3, 5, 7),
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid)
```

```{r}
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(1)
  
  # train model
  gbm.tune <- gbm(
    formula = medv ~ .,
    distribution = "gaussian",
    data = boston_train,
    n.trees = 2000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    train.fraction = .75, # train only on 0.75% of train subset and evaluate on remaining 0.25%
    n.cores = NULL # will use all cores by default
    #verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error)) 
}
```

```{r}
hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r}
set.seed(1)
bestTreeForPrediction = 104
yhat.boost <- gbm(medv~., data = boston_train,
                    distribution = "gaussian",
                    n.trees=bestTreeForPrediction,
                    interaction.depth=8,
                    shrinkage = 0.1,
                    n.minobsinnode = 7,
                    verbose=F)

yhat.boost <- predict(yhat.boost, boston_test, n.trees = bestTreeForPrediction)
mean((yhat.boost-boston_test$medv)^2)
```

```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
    nrounds = seq(40,150,5),
    max_depth = c(1,2,3,4,5,6),
    eta = seq(2,30,5)/60, 
    gamma = 1,
    colsample_bytree = 1.0,  
    min_child_weight = 1,
    subsample = 1.0
)

# in general you can fix eta (high) and number of rounds and tune other paramters (to increase speed),
# then come back to them and tune eta and nrounds
```

```{r}
xgb_control <- trainControl(
  method="cv", #re-samoling methods: "boot", "boot632", "optimism_boot", "boot_all","repeatedcv", "LOOCV", "LGOCV"
  number = 3
)

set.seed(1)
boston.xgb.tuned <- train(medv~., data=boston_train, 
                          trControl=xgb_control,
                          tuneGrid=hyper_grid,
                          lambda=0,
                          method="xgbTree")
```

```{r}
boston.xgb.tuned$bestTune
```

```{r}
plot(boston.xgb.tuned)
```

```{r}
boston.xgb = xgb.train(param = boston.xgb.tuned$bestTune,
                       data=dtrain,
                       nround = 115)

yhat.xgb <- predict(boston.xgb,X_test)
round(mean((yhat.xgb - boston_test$medv)^2),2)
```