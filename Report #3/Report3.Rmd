---
title: "Credit Card Report 3"
output:
  pdf_document: default
---

\begin{center}{\LARGE{\textbf{Third Report}}}\end{center}
Libraries and code used to set up the third report
```{r message = FALSE, fig.align='center'}
library(ISLR)
library(AER)
library(ggplot2)
library(dplyr)
library(glmnet)
data("CreditCard")
CreditCard = data.frame(CreditCard)
CreditCard$card <- ifelse(CreditCard$card=="yes", 1, 0)
CreditCard$owner <- ifelse(CreditCard$owner=="yes", 1, 0)
CreditCard$selfemp <- ifelse(CreditCard$selfemp=="yes", 1, 0)
```

\textbf{3a. Splitting dataset}
```{r message = FALSE}
library(caret)
library(dplyr)
library(glmnet)
data("CreditCard")
CreditCard = data.frame(CreditCard)
CreditCard$card <- ifelse(CreditCard$card=="yes", 1, 0)
CreditCard$owner <- ifelse(CreditCard$owner=="yes", 1, 0)
CreditCard$selfemp <- ifelse(CreditCard$selfemp=="yes", 1, 0)

set.seed(123)
train = CreditCard %>% sample_frac(.7)
test = CreditCard %>% setdiff(train)
x_train = model.matrix(card~., train)[,-1] 
x_test = model.matrix(card~., test)[,-1]
y_train = train$card
y_test = test$card

x = model.matrix(card~., CreditCard)[,-1] 
y = CreditCard$card
```
With this part, I divided 70 percent of my data into a training dataset and 30 percent to testing subset. \newline

\textbf{3b. Ridge Regression}
```{r}
grid = 10^seq(10, -2, length = 100)
ridge_mod = glmnet(x_train, y_train, alpha=0, lambda = grid, family = "binomial")
ridge_pred = predict(ridge_mod, s = 4, newx = x_test)
y_pred = ifelse(ridge_pred>0.2,1,0)
ridgeperf_matrix = table(y_pred, y_test)
ridgeperf = (sum(diag(ridgeperf_matrix)))/sum(ridgeperf_matrix)
ridgeperf
```
Before tuning my Ridge regression and using s = 4, I get a classification rate of 21% which is very bad. 
```{r message = FALSE, out.width="75%"}
set.seed(123)
ridge_cv.out = cv.glmnet(x_train, y_train, alpha = 0, family = "binomial")
bestlam = ridge_cv.out$lambda.min  
bestlam

plot(ridge_cv.out)
ridge_pred = predict(ridge_mod, s = bestlam, newx = x_test) 
y_pred = ifelse(ridge_pred>0.2,1,0)
out = glmnet(x, y, alpha = 0, family = "binomial")
ridge_coef = predict(out, type = "coefficients", s = bestlam)[1:12,] 
plot(out, xvar = "lambda")
```
```{r}
ridge_coef
```
```{r}
ridge_matrix = table(y_pred, y_test)
ridgeperf = (sum(diag(ridge_matrix)))/sum(ridge_matrix)
ridgeperf
bestlam
ridge_matrix
```

The $\lambda$ within 1 standard error is 0.01384886 in my Ridge Regression. 
The graph shows the relationship between the cross-validation error and log of $\lambda$ which is selected. The dash
is the minimum lambda. 
This is a plot of the coefficients of the variables. We see how with Ridge Regression, we are not decreasing the number of variables which is why they are all 11 on top. 
While most of these coefficients are small, the largest coefficients are dependents and income. 
Furthermore, we see the improvement tuning does on our model when we originally had 21% of our testing data correctly classified as opposed to 92% after tuning. \newline


\textbf{3c. Lasso Regression}

```{r message = FALSE, out.width="75%", fig.align='center'}

grid = 10^seq(10, -2, length = 100)
lasso_mod = glmnet(x_train, y_train, alpha=1, lambda = grid, family = "binomial")
lasso_pred = predict(lasso_mod, s = 4, newx = x_test)
y_pred = ifelse(lasso_pred>0.2,1,0)
lassoperf_matrix = table(y_pred, y_test)
lassoperf = (sum(diag(lassoperf_matrix)))/sum(lassoperf_matrix)
lassoperf
```

```{r message = FALSE, out.width="75%", fig.align='center'}
set.seed(123)
lasso_mod = glmnet(x_train, 
                   y_train, 
                   alpha = 1,
                   family = "binomial")
ls(lasso_mod)
plot(lasso_mod,  xvar = "lambda")

set.seed(1)
lasso_cv.out = cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
plot(lasso_cv.out)

bestlam = lasso_cv.out$lambda.1se
lasso_pred = predict(lasso_mod, s = bestlam, newx = x_test) 
y_pred = ifelse(lasso_pred>0.2,1,0)
lassoperf_matrix = table(y_pred, y_test)
lassoperf = (sum(diag(lassoperf_matrix)))/sum(lassoperf_matrix)
lassoperf
bestlam
out = glmnet(x, y, alpha = 1, lambda = grid, family = "binomial") 
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:12,] 
lasso_coef
lasso_coef[lasso_coef != 0]
```
```{r}
lasso_matrix = table(y_pred, y_test)
lassoperf = (sum(diag(lasso_matrix)))/sum(lasso_matrix)
lassoperf
bestlam
```
Lasso Regression has a 97.9% classification rate. Furthermore, we see how many of the coefficients in the graph are at zero, which is what Lasso regression does. \newline

\textbf{3d. Decision Tree}
```{r message = FALSE, out.width="75%", fig.align='center'}
data("CreditCard")
CreditCard = data.frame(CreditCard)
CreditCard$owner <- ifelse(CreditCard$owner=="yes", 1, 0)
CreditCard$selfemp <- ifelse(CreditCard$selfemp=="yes", 1, 0)
CreditCard$card = as.factor(CreditCard$card)

train = CreditCard %>% sample_frac(.7)
test = CreditCard %>% setdiff(train)

library(tree)
tree_card=tree(card~., data =train)
plot(tree_card)
text(tree_card, pretty = 0)
```
This tree has a depth of 3 and 4 leaves.
```{r message = FALSE, out.width = "75%", fig.align='center'}
tree_pred = predict(tree_card, test, type = "class")
tree_table = table(tree_pred, test$card)
treeperf = (sum(diag(tree_table)))/sum(tree_table)
treeperf
cv.card = cv.tree(tree_card, FUN = prune.misclass)
plot(cv.card$size, cv.card$dev, type = "b")
title("Credit Card Acceptance Decision Tree")
```
The decision tree for the data we have is very simple and it looks like it does not need to be pruned any more to achieve accurate results.
```{r}
tree_table = table(tree_pred, test$card)
treeperf = (sum(diag(tree_table)))/sum(tree_table)
treeperf
tree_table
```
```{r message = FALSE, warning = FALSE, out.width="75%", fig.align='center'}
prune_card = prune.misclass(tree_card, best = 5)
plot(prune_card)
text(prune_card, pretty = 0)
tree_pred = predict(prune_card, test, type = "class")
prune_table = table(tree_pred, test$card)
treeperf = (sum(diag(prune_table)))/sum(prune_table)
treeperf
```
When trying to prune our decision tree, we get the same tree which shows that the tree will not need any more splits and already performs as well as it can. 

```{r}
mlperformance<-matrix(c(ridgeperf, lassoperf, treeperf),ncol=1,
                      byrow=TRUE)
rownames(mlperformance)<-c("Ridge", "Lasso", "Decision Tree")
colnames(mlperformance)<-c("Performance")
mlperformance <- as.table(mlperformance)
mlperformance
```